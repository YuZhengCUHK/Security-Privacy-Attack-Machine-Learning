Privacy Attacks

Stealing Machine Learning Models via Prediction APIs
Model Reconstruction from Model Explanations
Membership Inference Attacks Against Machine Learning Models

Poisoning Attacks

Poisoning Attacks against Support Vector Machines
Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks
Stronger Data Poisoning Attacks Break Data Sanitization Defenses
Transferable Clean-Label Poisoning Attacks on Deep Neural Nets

Evasion Attacks (Adversarial Examples)

Explaining and Harnessing Adversarial Examples
Towards Evaluating the Robustness of Neural Networks
Why Do Adversarial Attacks Transfer? Explaining Transferability of Evasion and Poisoning Attacks
Defense against Poisoning Attacks

Certified Defenses for Data Poisoning Attacks
Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels
Robust Logistic Regression and Classification
Advanced Adversarial Attacks

Understanding Black-box Predictions via Influence Functions
Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent
Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning

Privacy Defenses

Machine Learning with Membership Privacy using Adversarial Regularization
Privacy-preserving Prediction
Deep Learning with Differential Privacy
Week 9 - Defenses against Adversarial Examples

Towards Deep Learning Models Resistant to Adversarial Attacks
Certified Defenses against Adversarial Examples
An abstract domain for certifying neural networks

Advanced topics on Adversarial Examples

Adversarially Robust Generalization Requires More Data
Adversarial Examples Are Not Bugs, They Are Features
Theoretically Principled Trade-off between Robustness and Accuracy
